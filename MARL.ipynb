{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71071e0-387b-4ce4-8308-9f3d6ee9353a",
   "metadata": {},
   "source": [
    "# Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac2f7f-7b83-427c-9f8c-750d64a0a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# Creates env\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    # Get observation and reward of the agent\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Randomly selected action from the action space\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eed9e-b28f-490b-8444-1573a5788000",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab570e88-c1f9-47df-a477-7f7f662d7e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons = 20)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a308f2-f19e-4d57-8227-9835c61fe16b",
   "metadata": {},
   "source": [
    "# Wrapping Environment for Fully Centralized Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f722ee79-b703-42ee-8b2d-29b13b67aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SingleAgentWrapperEnv(gymnasium.Env) :\n",
    "    \"\"\"\n",
    "    This wrapper permits to create a gymnasium env where the action space is the cartesian product of agents' action space\n",
    "    and the observation space is the cartesian product of agents' observation space.\n",
    "\n",
    "    This permits to train a single \"super-agent\" which will receive all observations and distribute actions that has to be made by all \"sub-agents\".\n",
    "\n",
    "    WARNING : USE THIS WRAPPER ONLY IF YOU FOLLOW ALL ASSUMPTIONS BELOW !!!\n",
    "     - The env is a pettingzoo env with the parallel API (ParallelEnv) (can be wrapped)\n",
    "     - All agents of the env has a Box action space and a Box observation space\n",
    "     - All agents of the env must have the same bounds, dtypes and a shape of (1,) for the Box representing their action space\n",
    "     - All agents of the env must have the same bounds, dtypes and a 1D shape of same size for the Box space representing their observation space\n",
    "\n",
    "\n",
    "    Do not forget to use env.reset() before creating SingleAgentWrapperEnv(env) to properly initialize all attributes of the env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, options = None) :\n",
    "        super(SingleAgentWrapperEnv, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.agents = self.env.unwrapped.agents\n",
    "        self.nb_agent = len(self.agents)\n",
    "\n",
    "\n",
    "        # Creating observation_space\n",
    "        temp_space = self.env.observation_space(self.agents[0])\n",
    "        low_bound = min(temp_space.low) # Not optimized this using min instead of taking the lowest value for each element grows the observation_space\n",
    "        high_bound = max(temp_space.high) # Same here with the max\n",
    "        dtype = temp_space.dtype\n",
    "\n",
    "        shape = (self.nb_agent * temp_space.shape[0],)\n",
    "        self.observation_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "        \n",
    "        # Creating action_space\n",
    "        temp_space = self.env.action_space(self.agents[0])\n",
    "        low_bound = temp_space.low # Lowest value an action of an agent can take\n",
    "        high_bound = temp_space.high # Highest value an action of an agent can take\n",
    "        dtype = temp_space.dtype # The exact type of an action\n",
    "        \n",
    "        shape = (self.nb_agent,) # A vector container at index i the action made by self.env.agents[i]\n",
    "        self.action_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        observations, infos = self.env.reset(seed, options)\n",
    "\n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "        return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        The action in input is a vector containing actions of each agents\n",
    "        \"\"\"\n",
    "        dict_actions = {self.agents[i]: np.array(object = [action[i]], dtype=np.float32) for i in range(self.nb_agent)}\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(dict_actions)\n",
    "\n",
    "        \n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        termination = False\n",
    "        truncation = False\n",
    "        for agent in self.agents :\n",
    "            # Reward is the mean of all agents' reward\n",
    "            reward += rewards[agent]\n",
    "            reward = reward / self.nb_agent\n",
    "\n",
    "            # Episode ends as soon as it ends for one agent\n",
    "            if terminations[agent] :\n",
    "                termination = True\n",
    "\n",
    "            if truncations[agent] :\n",
    "                truncation = True\n",
    "\n",
    "        return obs, reward, termination or len(self.agents) != self.nb_agent, truncation, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afefbc-8d86-4a52-a238-f607ea6b5fbe",
   "metadata": {},
   "source": [
    "### Use Grey Scale Image as observation and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da4aa78-624a-4600-a528-a79c746e3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "from supersuit import color_reduction_v0\n",
    "from supersuit import resize_v1\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "grey_scale_env = color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = resize_v1(grey_scale_env, 114, 30) # Divide by 4 quality of each image observed by each agent\n",
    "grey_scale_env = flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0ac2-c3d4-40f2-85fd-fec266b2e2aa",
   "metadata": {},
   "source": [
    "### Testing the wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adca4965-c10d-4d31-b8d8-6ee13604cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (51300,), uint8)\n",
      "Test of the Wrapped Environment is done.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import supersuit\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "check_env(wrappedEnv)\n",
    "\n",
    "while env.aec_env.agents :\n",
    "    # this is where you would insert your policy\n",
    "    actions = [env.action_space(env.agents[0]).sample()[0] for agent in env.aec_env.agents]\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = wrappedEnv.step(actions)\n",
    "\n",
    "env.close()\n",
    "print(wrappedEnv.observation_space)\n",
    "print(\"Test of the Wrapped Environment is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547fc2-a233-444a-a899-b9485797753f",
   "metadata": {},
   "source": [
    "### Initializing PPO agent on wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85161c0a-7673-41db-91ea-b484a996f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(n_pistons = 10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "\n",
    "# Initializing PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrappedEnv)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e15778-95bf-4bdc-b8e6-929b7220eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e733e4d-b547-44f0-b7e3-31ce63ecd1ec",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc05732-b267-4dfe-a624-33ccd600a69e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/lambda_wrappers/observation_lambda.py:68\u001b[39m, in \u001b[36maec_observation_lambda._modify_observation\u001b[39m\u001b[34m(self, agent, observation)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchange_observation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_obs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: basic_obs_wrapper.<locals>.change_obs() takes 2 positional arguments but 3 were given",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Took 1 hour for 30000 steps without reducing image size for 10 pistons\u001b[39;00m\n\u001b[32m      3\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mSingleAgentWrapperEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03mThe action in input is a vector containing actions of each agents\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m dict_actions = {\u001b[38;5;28mself\u001b[39m.agents[i]: np.array(\u001b[38;5;28mobject\u001b[39m = [action[i]], dtype=np.float32) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.nb_agent)}\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m observations, rewards, terminations, truncations, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m obs = np.array([], dtype = \u001b[38;5;28mself\u001b[39m.observation_space.dtype)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.nb_agent) :\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/generic_wrappers/utils/shared_wrapper_util.py:130\u001b[39m, in \u001b[36mshared_wrapper_parr.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[32m    126\u001b[39m     actions = {\n\u001b[32m    127\u001b[39m         agent: \u001b[38;5;28mself\u001b[39m.modifiers[agent].modify_action(action)\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent, action \u001b[38;5;129;01min\u001b[39;00m actions.items()\n\u001b[32m    129\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     observations, rewards, terminations, truncations, infos = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_modifiers(\u001b[38;5;28mself\u001b[39m.agents)\n\u001b[32m    132\u001b[39m     observations = {\n\u001b[32m    133\u001b[39m         agent: \u001b[38;5;28mself\u001b[39m.modifiers[agent].modify_obs(obs)\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent, obs \u001b[38;5;129;01min\u001b[39;00m observations.items()\n\u001b[32m    135\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base_parallel.py:34\u001b[39m, in \u001b[36mBaseParallelWrapper.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m, actions: \u001b[38;5;28mdict\u001b[39m[AgentID, ActionType]\n\u001b[32m     27\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[38;5;28mdict\u001b[39m[AgentID, \u001b[38;5;28mdict\u001b[39m],\n\u001b[32m     33\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pettingzoo/utils/conversions.py:206\u001b[39m, in \u001b[36maec_to_parallel_wrapper.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.aec_env.agent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m obs, rew, termination, truncation, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maec_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.aec_env.step(actions[agent])\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aec_env.agents:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pettingzoo/utils/env.py:186\u001b[39m, in \u001b[36mAECEnv.last\u001b[39m\u001b[34m(self, observe)\u001b[39m\n\u001b[32m    184\u001b[39m agent = \u001b[38;5;28mself\u001b[39m.agent_selection\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m observe \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    188\u001b[39m     observation,\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m._cumulative_rewards[agent],\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m.infos[agent],\n\u001b[32m    193\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/utils/base_aec_wrapper.py:35\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     obs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# problem is in this line, the obs is sometimes a different size from the obs space\u001b[39;00m\n\u001b[32m     38\u001b[39m     observation = \u001b[38;5;28mself\u001b[39m._modify_observation(agent, obs)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:101\u001b[39m, in \u001b[36mOrderEnforcingWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    100\u001b[39m     EnvLogger.error_observe_before_reset()\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base.py:41\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) -> ObsType | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/utils/base_aec_wrapper.py:38\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[32m     35\u001b[39m     obs = \u001b[38;5;28msuper\u001b[39m().observe(\n\u001b[32m     36\u001b[39m         agent\n\u001b[32m     37\u001b[39m     )  \u001b[38;5;66;03m# problem is in this line, the obs is sometimes a different size from the obs space\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modify_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/lambda_wrappers/observation_lambda.py:70\u001b[39m, in \u001b[36maec_observation_lambda._modify_observation\u001b[39m\u001b[34m(self, agent, observation)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.change_observation_fn(observation, old_obs_space, agent)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchange_observation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_obs_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/generic_wrappers/basic_wrappers.py:19\u001b[39m, in \u001b[36mbasic_obs_wrapper.<locals>.change_obs\u001b[39m\u001b[34m(obs, obs_space)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchange_obs\u001b[39m(obs, obs_space):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchange_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/supersuit/utils/basic_transforms/resize.py:28\u001b[39m, in \u001b[36mchange_observation\u001b[39m\u001b[34m(obs, obs_space, resize)\u001b[39m\n\u001b[32m     26\u001b[39m     obs = obs.reshape(obs.shape + (\u001b[32m1\u001b[39m,))\n\u001b[32m     27\u001b[39m interp_method = \u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m linear_interp \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m obs = \u001b[43mtinyscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterp_method\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obs_space.shape) == \u001b[32m2\u001b[39m:\n\u001b[32m     32\u001b[39m     obs = obs.reshape(obs.shape[:\u001b[32m2\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=30000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bd1e1-799e-4964-8302-2a1f4bafa92d",
   "metadata": {},
   "source": [
    "### Save PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb09a4b-b822-453f-8bc8-430c0057a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df32c-f968-46f5-be85-241ccef75ae3",
   "metadata": {},
   "source": [
    "### Import PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f4d8d3-1bf0-4eaa-b8ff-ea4846d195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413040ca-0192-46b3-9657-d237ca6f0ecb",
   "metadata": {},
   "source": [
    "### Test PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35097e7c-63e7-4653-9d3b-f095e24bdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons=10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "vec_env = make_vec_env(lambda : wrappedEnv, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2466-994d-4c0c-bd55-abda258edf98",
   "metadata": {},
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7062-5d6a-43b0-ba3e-42eaddc1f60b",
   "metadata": {},
   "source": [
    "All pistons have the same goal, same rewards and same observations.\n",
    "\n",
    "Instead of training each pistons or training a single agent controlling all pistons, we could consider all pistons being the same one.\n",
    "\n",
    "In this case we would use all observations and rewards from all pistons to train only a single piston.\n",
    "And then treat all pistons as being a copy of this single piston.\n",
    "\n",
    "\n",
    "In fact parameter sharing cannot work here based on how rewards are distributed in this environment : same reward for each agent. Thus useless pistons (and actions they made) will have the same impact (even a bigger impact since they are more than usefull ones) as usefull pistons which make the ball going to the left wall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c271-c22c-4f83-84c0-7cb420981466",
   "metadata": {},
   "source": [
    "### Initialize PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56591909-3792-4d6d-aa44-38906707b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (30, 114, 5), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import numpy as np\n",
    "import supersuit\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode = None, n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "#model = PPO(\"CnnPolicy\", env)\n",
    "model = PPO(\"MlpPolicy\", env)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c549bc5a-0e26-436e-845a-fcbaefdc8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc67a6d-b332-4a0d-92ad-7093ab3adab2",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b756427-c7de-4992-a206-57b750cb7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f42dd-9792-42e9-ade0-320071a30851",
   "metadata": {},
   "source": [
    "### Test PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66859-9e1d-41ee-b130-c09f36649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "import supersuit\n",
    "\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode = \"human\", n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "\n",
    "#vec_env = make_vec_env(lambda : env, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = np.array([False])\n",
    "while not done.any() :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777553b-1501-488a-b0fe-be138eee5474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
