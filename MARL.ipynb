{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71071e0-387b-4ce4-8308-9f3d6ee9353a",
   "metadata": {},
   "source": [
    "# Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ac2f7f-7b83-427c-9f8c-750d64a0a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# Creates env\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    # Get observation and reward of the agent\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Randomly selected action from the action space\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eed9e-b28f-490b-8444-1573a5788000",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab570e88-c1f9-47df-a477-7f7f662d7e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons = 20)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a308f2-f19e-4d57-8227-9835c61fe16b",
   "metadata": {},
   "source": [
    "# Wrapping Environment for Fully Centralized Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f722ee79-b703-42ee-8b2d-29b13b67aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SingleAgentWrapperEnv(gymnasium.Env) :\n",
    "    \"\"\"\n",
    "    This wrapper permits to create a gymnasium env where the action space is the cartesian product of agents' action space\n",
    "    and the observation space is the cartesian product of agents' observation space.\n",
    "\n",
    "    This permits to train a single \"super-agent\" which will receive all observations and distribute actions that has to be made by all \"sub-agents\".\n",
    "\n",
    "    WARNING : USE THIS WRAPPER ONLY IF YOU FOLLOW ALL ASSUMPTIONS BELOW !!!\n",
    "     - The env is a pettingzoo env with the parallel API (ParallelEnv) (can be wrapped)\n",
    "     - All agents of the env has a Box action space and a Box observation space\n",
    "     - All agents of the env must have the same bounds, dtypes and a shape of (1,) for the Box representing their action space\n",
    "     - All agents of the env must have the same bounds, dtypes and a 1D shape of same size for the Box space representing their observation space\n",
    "\n",
    "\n",
    "    Do not forget to use env.reset() before creating SingleAgentWrapperEnv(env) to properly initialize all attributes of the env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, options = None) :\n",
    "        super(SingleAgentWrapperEnv, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.agents = self.env.unwrapped.agents\n",
    "        self.nb_agent = len(self.agents)\n",
    "\n",
    "\n",
    "        # Creating observation_space\n",
    "        temp_space = self.env.observation_space(self.agents[0])\n",
    "        low_bound = min(temp_space.low) # Not optimized this using min instead of taking the lowest value for each element grows the observation_space\n",
    "        high_bound = max(temp_space.high) # Same here with the max\n",
    "        dtype = temp_space.dtype\n",
    "\n",
    "        shape = (self.nb_agent * temp_space.shape[0],)\n",
    "        self.observation_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "        \n",
    "        # Creating action_space\n",
    "        temp_space = self.env.action_space(self.agents[0])\n",
    "        low_bound = temp_space.low # Lowest value an action of an agent can take\n",
    "        high_bound = temp_space.high # Highest value an action of an agent can take\n",
    "        dtype = temp_space.dtype # The exact type of an action\n",
    "        \n",
    "        shape = (self.nb_agent,) # A vector container at index i the action made by self.env.agents[i]\n",
    "        self.action_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        observations, infos = self.env.reset(seed, options)\n",
    "\n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "        return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        The action in input is a vector containing actions of each agents\n",
    "        \"\"\"\n",
    "        dict_actions = {self.agents[i]: np.array(object = [action[i]], dtype=np.float32) for i in range(self.nb_agent)}\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(dict_actions)\n",
    "\n",
    "        \n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        termination = False\n",
    "        truncation = False\n",
    "        for agent in self.agents :\n",
    "            # Reward is the mean of all agents' reward\n",
    "            reward += rewards[agent]\n",
    "            reward = reward / self.nb_agent\n",
    "\n",
    "            # Episode ends as soon as it ends for one agent\n",
    "            if terminations[agent] :\n",
    "                termination = True\n",
    "\n",
    "            if truncations[agent] :\n",
    "                truncation = True\n",
    "\n",
    "        return obs, reward, termination or len(self.agents) != self.nb_agent, truncation, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afefbc-8d86-4a52-a238-f607ea6b5fbe",
   "metadata": {},
   "source": [
    "### Use Grey Scale Image as observation and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da4aa78-624a-4600-a528-a79c746e3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "from supersuit import color_reduction_v0\n",
    "from supersuit import resize_v1\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "grey_scale_env = color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = resize_v1(grey_scale_env, 114, 30) # Divide by 4 quality of each image observed by each agent\n",
    "grey_scale_env = flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0ac2-c3d4-40f2-85fd-fec266b2e2aa",
   "metadata": {},
   "source": [
    "### Testing the wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adca4965-c10d-4d31-b8d8-6ee13604cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (51300,), uint8)\n",
      "Test of the Wrapped Environment is done.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import supersuit\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "check_env(wrappedEnv)\n",
    "\n",
    "while env.aec_env.agents :\n",
    "    # this is where you would insert your policy\n",
    "    actions = [env.action_space(env.agents[0]).sample()[0] for agent in env.aec_env.agents]\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = wrappedEnv.step(actions)\n",
    "\n",
    "env.close()\n",
    "print(wrappedEnv.observation_space)\n",
    "print(\"Test of the Wrapped Environment is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547fc2-a233-444a-a899-b9485797753f",
   "metadata": {},
   "source": [
    "### Initializing PPO agent on wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85161c0a-7673-41db-91ea-b484a996f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(n_pistons = 10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "\n",
    "# Initializing PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrappedEnv)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e15778-95bf-4bdc-b8e6-929b7220eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e733e4d-b547-44f0-b7e3-31ce63ecd1ec",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc05732-b267-4dfe-a624-33ccd600a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bd1e1-799e-4964-8302-2a1f4bafa92d",
   "metadata": {},
   "source": [
    "### Save PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb09a4b-b822-453f-8bc8-430c0057a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df32c-f968-46f5-be85-241ccef75ae3",
   "metadata": {},
   "source": [
    "### Import PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f4d8d3-1bf0-4eaa-b8ff-ea4846d195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413040ca-0192-46b3-9657-d237ca6f0ecb",
   "metadata": {},
   "source": [
    "### Test PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35097e7c-63e7-4653-9d3b-f095e24bdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons=10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "vec_env = make_vec_env(lambda : wrappedEnv, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2466-994d-4c0c-bd55-abda258edf98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7062-5d6a-43b0-ba3e-42eaddc1f60b",
   "metadata": {},
   "source": [
    "All pistons have the same goal, same rewards and same observations.\n",
    "\n",
    "Instead of training each pistons or training a single agent controlling all pistons, we could consider all pistons being the same one.\n",
    "\n",
    "In this case we would use all observations and rewards from all pistons to train only a single piston.\n",
    "And then treat all pistons as being a copy of this single piston.\n",
    "\n",
    "\n",
    "In fact parameter sharing cannot work here based on how rewards are distributed in this environment : same reward for each agent. Thus useless pistons (and actions they made) will have the same impact (even a bigger impact since they are more than usefull ones) as usefull pistons which make the ball going to the left wall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c271-c22c-4f83-84c0-7cb420981466",
   "metadata": {},
   "source": [
    "### Initialize PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56591909-3792-4d6d-aa44-38906707b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (30, 114, 5), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import numpy as np\n",
    "import supersuit\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode = None, n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "#model = PPO(\"CnnPolicy\", env)\n",
    "model = PPO(\"MlpPolicy\", env)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c549bc5a-0e26-436e-845a-fcbaefdc8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc67a6d-b332-4a0d-92ad-7093ab3adab2",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b756427-c7de-4992-a206-57b750cb7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f42dd-9792-42e9-ade0-320071a30851",
   "metadata": {},
   "source": [
    "### Test PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66859-9e1d-41ee-b130-c09f36649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "import supersuit\n",
    "\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode = \"human\", n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "\n",
    "#vec_env = make_vec_env(lambda : env, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = np.array([False])\n",
    "while not done.any() :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3255a-0d86-4d03-b121-02a3d5bf83a7",
   "metadata": {},
   "source": [
    "# RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bb471-b656-4dbf-aa23-657b7ae8e71c",
   "metadata": {},
   "source": [
    "### Register Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f651d9b-ab7f-4514-bd78-d40fd4c44bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-11-20 16:50:33,958\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m /home/enzo/Documents/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m 2025-11-20 16:50:52,907\tERROR actor_manager.py:186 -- Worker exception caught during `apply()`: Input type (unsigned char) and bias type (float) should be the same\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 182, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 333, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     to_env = self.module.forward_exploration(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 616, in forward_exploration\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._forward_exploration(batch, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/rl_module/multi_rl_module.py\", line 213, in _forward_exploration\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     mid: self._rl_modules[mid]._forward_exploration(batch[mid], **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 96, in _forward_exploration\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._forward(batch, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py\", line 31, in _forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     encoder_outs = self.encoder(batch)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py\", line 75, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._forward(inputs, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/base.py\", line 334, in _forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     actor_out = self.actor_encoder(inputs, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py\", line 75, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._forward(inputs, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py\", line 111, in _forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return {ENCODER_OUT: self.net(inputs[Columns.OBS])}\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     input = module(input)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m             ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py\", line 312, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     out = self.cnn(inputs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m           ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     input = module(input)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m             ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m     return F.conv2d(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m            ^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m RuntimeError: Input type (unsigned char) and bias type (float) should be the same\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-20 16:51:02,053 E 275184 275184] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m 2025-11-20 16:50:52,908\tERROR actor_manager.py:186 -- Worker exception caught during `apply()`: Input type (unsigned char) and bias type (float) should be the same\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 182, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m             ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 333, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     to_env = self.module.forward_exploration(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py\", line 616, in forward_exploration\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return self._forward_exploration(batch, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py\", line 96, in _forward_exploration\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     mid: self._rl_modules[mid]._forward_exploration(batch[mid], **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return self._forward(batch, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py\", line 111, in _forward\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     encoder_outs = self.encoder(batch)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return self._call_impl(*args, **kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return forward_call(*args, **kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 548, in forward\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return self._forward(inputs, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     actor_out = self.actor_encoder(inputs, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return {ENCODER_OUT: self.net(inputs[Columns.OBS])}\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     input = module(input)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     out = self.cnn(inputs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m           ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m   File \"/home/enzo/Documents/customEnv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m     return F.conv2d(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m            ^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275338)\u001b[0m RuntimeError: Input type (unsigned char) and bias type (float) should be the same\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-20 16:51:03,891 E 275283 275283] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275336)\u001b[0m [2025-11-20 16:51:06,957 E 275336 275395] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 000000001afceac94d16bb570e1144be8c386d3ef25f54486d26b374f0533b74 Worker ID: d390dcba3c3fd3f7a0d032c869bd0289398c69e8b578f101032d8b9f Node ID: 90904ca1f7aa5f1819a8ce8ff895d8df02244ce1596ed849fb971d7c Worker IP address: 134.206.153.65 Worker port: 35551 Worker PID: 275336 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=275339)\u001b[0m /home/enzo/Documents/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275339)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=275339)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "import supersuit\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "env_name = \"pistonball_v6\"\n",
    "\n",
    "def env_creator(args) :\n",
    "    env = pistonball_v6.parallel_env(n_pistons=20)\n",
    "    env = supersuit.color_reduction_v0(env, mode='full') # image to black and white\n",
    "    env = supersuit.resize_v1(env, 84, 84) # Reduce size of image\n",
    "    env = supersuit.reshape_v0(env, env.observation_space('piston_0').shape + (1,))\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb53fb1-50d9-44c1-815e-1c116304a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(\"\")\n",
    "env.reset()\n",
    "print(env.observation_space('piston_0').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a31e-2f51-478a-bd3a-aec522c3ceef",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f972b88-298c-48bd-9300-0335c705cb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 16:50:43,074\tWARNING algorithm_config.py:5053 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:526: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/enzo/Documents/customEnv/lib/python3.12/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-11-20 16:50:43,180 E 275136 275136] core_worker.cc:2200: Actor with class name: 'MultiAgentEnvRunner' and ID: '074e422160230821ba80615901000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 16:50:47,506\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    .rl_module(\\n        rl_module_spec = MultiRLModuleSpec(rl_module_specs={\\n            agent_id : RLModuleSpec() for agent_id in env.unwrapped.agents\\n        })\\n    )\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name)\n",
    "    .multi_agent(policies = env.unwrapped.agents, policy_mapping_fn = lambda agent_id, episode, **kwargs : agent_id)\n",
    ")\n",
    "\n",
    "\n",
    "config.validate()\n",
    "print(config.is_multi_agent)\n",
    "\n",
    "algo = config.build_algo()\n",
    "\n",
    "\"\"\"\n",
    "    .rl_module(\n",
    "        rl_module_spec = MultiRLModuleSpec(rl_module_specs={\n",
    "            agent_id : RLModuleSpec() for agent_id in env.unwrapped.agents\n",
    "        })\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d1c8-36a7-49ce-a766-81245844af48",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd30e42b-714f-4f44-8877-36575516e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-20 16:51:08,150 E 275136 275334] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-20 16:51:52,878\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-11-20 16:51:54,155\tERROR actor_manager.py:973 -- Ray error (The actor 074e422160230821ba80615901000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-11-20 16:51:54,156\tERROR actor_manager.py:973 -- Ray error (The actor caa19bc5e6d43895a1f7222401000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-11-20 16:51:54,157\tERROR actor_manager.py:771 -- The actor 074e422160230821ba80615901000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-11-20 16:51:54,158\tERROR actor_manager.py:771 -- The actor caa19bc5e6d43895a1f7222401000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timers': {'training_iteration': 60.00750333699398,\n",
       "  'restore_env_runners': 3.207288682460785e-05,\n",
       "  'training_step': 60.007248636102304,\n",
       "  'env_runner_sampling_timer': 60.00712985591963},\n",
       " 'num_training_step_calls_per_iteration': 1,\n",
       " 'num_env_steps_sampled_lifetime': 0,\n",
       " 'fault_tolerance': {'num_healthy_workers': 0,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-11-20_16-51-54',\n",
       " 'timestamp': 1763653914,\n",
       " 'time_this_iter_s': 61.28780245780945,\n",
       " 'time_total_s': 61.28780245780945,\n",
       " 'pid': 275136,\n",
       " 'hostname': 'smac-Precision-Tower-3620',\n",
       " 'node_ip': '134.206.153.65',\n",
       " 'config': {'exploration_config': {},\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'torch_ddp_kwargs': {},\n",
       "  'torch_skip_nan_gradients': False,\n",
       "  'env': 'pistonball_v6',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'disable_env_checking': False,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'create_local_env_runner': True,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'gym_env_vectorize_mode': 'SYNC',\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'episodes_to_numpy': True,\n",
       "  'max_requests_in_flight_per_env_runner': 1,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'merge_env_runner_states': 'training_only',\n",
       "  'broadcast_env_runner_states': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  '_is_online': True,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 'auto',\n",
       "  'num_aggregator_actors_per_learner': 0,\n",
       "  'max_requests_in_flight_per_aggregator_actor': 3,\n",
       "  'local_gpu_idx': 0,\n",
       "  'max_requests_in_flight_per_learner': 3,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  '_train_batch_size_per_learner': None,\n",
       "  'train_batch_size': 4000,\n",
       "  'num_epochs': 30,\n",
       "  'minibatch_size': 128,\n",
       "  'shuffle_batch_per_epoch': True,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'log_std_clip_param': 20.0,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  '_learner_class': None,\n",
       "  'callbacks_on_algorithm_init': None,\n",
       "  'callbacks_on_env_runners_recreated': None,\n",
       "  'callbacks_on_offline_eval_runners_recreated': None,\n",
       "  'callbacks_on_checkpoint_loaded': None,\n",
       "  'callbacks_on_environment_created': None,\n",
       "  'callbacks_on_episode_created': None,\n",
       "  'callbacks_on_episode_start': None,\n",
       "  'callbacks_on_episode_step': None,\n",
       "  'callbacks_on_episode_end': None,\n",
       "  'callbacks_on_evaluate_start': None,\n",
       "  'callbacks_on_evaluate_end': None,\n",
       "  'callbacks_on_evaluate_offline_start': None,\n",
       "  'callbacks_on_evaluate_offline_end': None,\n",
       "  'callbacks_on_sample_end': None,\n",
       "  'callbacks_on_train_result': None,\n",
       "  'explore': True,\n",
       "  'enable_rl_module_and_learner': True,\n",
       "  'enable_env_runner_and_connector_v2': True,\n",
       "  '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function __main__.<lambda>(agent_id, episode, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'offline_data_class': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_read_sample_batches': False,\n",
       "  'input_read_batch_size': None,\n",
       "  'input_filesystem': None,\n",
       "  'input_filesystem_kwargs': {},\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'materialize_data': False,\n",
       "  'materialize_mapped_data': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'ignore_final_observation': False,\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_buffer_class': None,\n",
       "  'prelearner_buffer_kwargs': {},\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_remaining_data': False,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
       "  'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'offline_evaluation_interval': None,\n",
       "  'num_offline_eval_runners': 0,\n",
       "  'offline_evaluation_type': None,\n",
       "  'offline_eval_runner_class': None,\n",
       "  'offline_loss_for_module_fn': None,\n",
       "  'offline_evaluation_duration': 1,\n",
       "  'offline_evaluation_parallel_to_training': False,\n",
       "  'offline_evaluation_timeout_s': 120.0,\n",
       "  'num_cpus_per_offline_eval_runner': 1,\n",
       "  'num_gpus_per_offline_eval_runner': 0,\n",
       "  'custom_resources_per_offline_eval_runner': {},\n",
       "  'restart_failed_offline_eval_runners': True,\n",
       "  'ignore_offline_eval_runner_failures': False,\n",
       "  'max_num_offline_eval_runner_restarts': 1000,\n",
       "  'offline_eval_runner_restore_timeout_s': 1800.0,\n",
       "  'max_requests_in_flight_per_offline_eval_runner': 1,\n",
       "  'validate_offline_eval_runners_after_construction': True,\n",
       "  'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
       "  'offline_eval_rl_module_inference_only': False,\n",
       "  'broadcast_offline_eval_runner_states': False,\n",
       "  'offline_eval_batch_size_per_runner': 256,\n",
       "  'dataset_num_iters_per_eval_runner': 1,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'log_gradients': False,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'restart_failed_env_runners': True,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30.0,\n",
       "  'env_runner_restore_timeout_s': 1800.0,\n",
       "  '_model_config': {},\n",
       "  '_rl_module_spec': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_validate_config': True,\n",
       "  '_use_msgpack_checkpoints': False,\n",
       "  '_torch_grad_scaler_class': None,\n",
       "  '_torch_lr_scheduler_classes': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'env_task_fn': -1,\n",
       "  'enable_connectors': -1,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'lr_schedule': None,\n",
       "  'sgd_minibatch_size': -1,\n",
       "  'vf_share_layers': -1,\n",
       "  'class': ray.rllib.algorithms.ppo.ppo.PPOConfig,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'piston_0': (None, None, None, None),\n",
       "   'piston_1': (None, None, None, None),\n",
       "   'piston_2': (None, None, None, None),\n",
       "   'piston_3': (None, None, None, None),\n",
       "   'piston_4': (None, None, None, None),\n",
       "   'piston_5': (None, None, None, None),\n",
       "   'piston_6': (None, None, None, None),\n",
       "   'piston_7': (None, None, None, None),\n",
       "   'piston_8': (None, None, None, None),\n",
       "   'piston_9': (None, None, None, None),\n",
       "   'piston_10': (None, None, None, None),\n",
       "   'piston_11': (None, None, None, None),\n",
       "   'piston_12': (None, None, None, None),\n",
       "   'piston_13': (None, None, None, None),\n",
       "   'piston_14': (None, None, None, None),\n",
       "   'piston_15': (None, None, None, None),\n",
       "   'piston_16': (None, None, None, None),\n",
       "   'piston_17': (None, None, None, None),\n",
       "   'piston_18': (None, None, None, None),\n",
       "   'piston_19': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.callbacks.callbacks.RLlibCallback,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 61.28780245780945,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(18.074193548387097),\n",
       "  'ram_util_percent': np.float64(48.6010752688172)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28716cc8-6643-4a32-8659-867b4cc31f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x79d93822b080> doesn't have an env! Can't call `sample()` on it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:1481\u001b[39m, in \u001b[36mAlgorithm.evaluate\u001b[39m\u001b[34m(self, parallel_train_future)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;66;03m# There is no eval EnvRunnerGroup -> Run on local EnvRunner.\u001b[39;00m\n\u001b[32m   1475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env_runner:\n\u001b[32m   1476\u001b[39m     (\n\u001b[32m   1477\u001b[39m         eval_results,\n\u001b[32m   1478\u001b[39m         env_steps,\n\u001b[32m   1479\u001b[39m         agent_steps,\n\u001b[32m   1480\u001b[39m         batches,\n\u001b[32m-> \u001b[39m\u001b[32m1481\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_on_local_env_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1482\u001b[39m \u001b[38;5;66;03m# There is only a local eval EnvRunner -> Run on that.\u001b[39;00m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_env_runner_group.num_healthy_remote_workers() == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py:1636\u001b[39m, in \u001b[36mAlgorithm._evaluate_on_local_env_runner\u001b[39m\u001b[34m(self, env_runner)\u001b[39m\n\u001b[32m   1634\u001b[39m all_batches = []\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m-> \u001b[39m\u001b[32m1636\u001b[39m     episodes = \u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimesteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepisodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1639\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1640\u001b[39m     agent_steps += \u001b[38;5;28msum\u001b[39m(e.agent_steps() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m episodes)\n\u001b[32m   1641\u001b[39m     env_steps += \u001b[38;5;28msum\u001b[39m(e.env_steps() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m episodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py:182\u001b[39m, in \u001b[36mMultiAgentEnvRunner.sample\u001b[39m\u001b[34m(self, num_timesteps, num_episodes, explore, random_actions, force_reset)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs and returns a sample (n timesteps or m episodes) on the env(s).\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m \u001b[33;03m    A list of `MultiAgentEpisode` instances, carrying the sampled data.\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    183\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have an env! Can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt call `sample()` on it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (num_timesteps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_episodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), (\n\u001b[32m    187\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProvide \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33meither `num_timesteps` or `num_episodes`. Both provided here:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_timesteps\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m )\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Log time between `sample()` requests.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: <ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x79d93822b080> doesn't have an env! Can't call `sample()` on it."
     ]
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96dfec0-756a-4421-ae35-9637ba1b6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.stop()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249016e5-bd39-4ac2-b4ec-0d2bc3b1d345",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd90dbcc-7c24-4e34-a321-865533c53e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-11-14 16:35:45,612\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-11-14 16:35:49,137\tWARNING deprecation.py:50 -- DeprecationWarning: `config.training(num_sgd_iter=..)` has been deprecated. Use `config.training(num_epochs=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`policies` must be dict mapping PolicyID to PolicySpec OR a set/tuple/list of PolicyIDs!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m     80\u001b[39m register_env(env_name, _make_rllib_env)\n\u001b[32m     82\u001b[39m ModelCatalog.register_custom_model(\u001b[33m\"\u001b[39m\u001b[33mCNNModelV2\u001b[39m\u001b[33m\"\u001b[39m, CNNModelV2)\n\u001b[32m     84\u001b[39m config = (\n\u001b[32m     85\u001b[39m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_env_checking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gae\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_param\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvf_loss_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebugging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mERROR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRLLIB_NUM_GPUS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicies_to_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m )\n\u001b[32m    113\u001b[39m storage_uri = (Path(\u001b[33m\"\u001b[39m\u001b[33m~/ray_results\u001b[39m\u001b[33m\"\u001b[39m) / env_name).expanduser().resolve().as_uri()\n\u001b[32m    115\u001b[39m tune.run(\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPPO\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mPPO\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     config=config.to_dict(),\n\u001b[32m    122\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:3535\u001b[39m, in \u001b[36mAlgorithmConfig.multi_agent\u001b[39m\u001b[34m(self, policies, policy_map_capacity, policy_mapping_fn, policies_to_train, policy_states_are_swappable, observation_fn, count_steps_by, algorithm_config_overrides_per_module, replay_mode, policy_map_cache)\u001b[39m\n\u001b[32m   3533\u001b[39m         \u001b[38;5;28mself\u001b[39m.policies = policies\n\u001b[32m   3534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3535\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3536\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`policies` must be dict mapping PolicyID to PolicySpec OR a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3537\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mset/tuple/list of PolicyIDs!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3538\u001b[39m         )\n\u001b[32m   3540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m algorithm_config_overrides_per_module != DEPRECATED_VALUE:\n\u001b[32m   3541\u001b[39m     deprecation_warning(old=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, error=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: `policies` must be dict mapping PolicyID to PolicySpec OR a set/tuple/list of PolicyIDs!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-14 16:36:13,677 E 446246 446246] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-14 16:36:15,545 E 446344 446344] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(pid=446398)\u001b[0m [2025-11-14 16:36:18,704 E 446398 446463] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
    "\n",
    "Author: Rohan (https://github.com/Rohan138)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import ray\n",
    "import supersuit as ss\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "from torch import nn\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "\n",
    "class CNNModelV2(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
    "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            (nn.Linear(3136, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_fn = nn.Linear(512, num_outputs)\n",
    "        self.value_fn = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
    "        self._value_out = self.value_fn(model_out)\n",
    "        return self.policy_fn(model_out), state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self._value_out.flatten()\n",
    "\n",
    "\n",
    "def env_creator(args):\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        n_pistons=20,\n",
    "        time_penalty=-0.1,\n",
    "        continuous=True,\n",
    "        random_drop=True,\n",
    "        random_rotate=True,\n",
    "        ball_mass=0.75,\n",
    "        ball_friction=0.3,\n",
    "        ball_elasticity=1.5,\n",
    "        max_cycles=125,\n",
    "    )\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.dtype_v0(env, \"float32\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    return env\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ray.init()\n",
    "\n",
    "    env_name = \"pistonball_v6\"\n",
    "\n",
    "    def _make_rllib_env(config):\n",
    "        base = env_creator(config)\n",
    "        wrapped = ParallelPettingZooEnv(base)\n",
    "        wrapped._agent_ids = set(getattr(base, \"possible_agents\", []))\n",
    "        return wrapped\n",
    "\n",
    "    register_env(env_name, _make_rllib_env)\n",
    "\n",
    "    ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(\n",
    "            env=env_name,\n",
    "            clip_actions=True,\n",
    "            disable_env_checking=True,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size=512,\n",
    "            lr=2e-5,\n",
    "            gamma=0.99,\n",
    "            lambda_=0.9,\n",
    "            use_gae=True,\n",
    "            clip_param=0.4,\n",
    "            grad_clip=None,\n",
    "            entropy_coeff=0.1,\n",
    "            vf_loss_coeff=0.25,\n",
    "            num_sgd_iter=10,\n",
    "        )\n",
    "        .debugging(log_level=\"ERROR\")\n",
    "        .framework(framework=\"torch\")\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "        .multi_agent(\n",
    "            policies = \"learning_policy\",\n",
    "            policy_mapping_fn = 'learning_policy',\n",
    "            policies_to_train=[\"learning_policy\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    storage_uri = (Path(\"~/ray_results\") / env_name).expanduser().resolve().as_uri()\n",
    "\n",
    "    tune.run(\n",
    "        \"PPO\",\n",
    "        name=\"PPO\",\n",
    "        stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
    "        checkpoint_freq=10,\n",
    "        storage_path=storage_uri,\n",
    "        config=config.to_dict(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d490f-78e2-4130-a33e-688a4210baa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
