{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71071e0-387b-4ce4-8308-9f3d6ee9353a",
   "metadata": {},
   "source": [
    "# Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ac2f7f-7b83-427c-9f8c-750d64a0a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# Creates env\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    # Get observation and reward of the agent\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Randomly selected action from the action space\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eed9e-b28f-490b-8444-1573a5788000",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab570e88-c1f9-47df-a477-7f7f662d7e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons = 20)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a308f2-f19e-4d57-8227-9835c61fe16b",
   "metadata": {},
   "source": [
    "# Wrapping Environment for Fully Centralized Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f722ee79-b703-42ee-8b2d-29b13b67aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SingleAgentWrapperEnv(gymnasium.Env) :\n",
    "    \"\"\"\n",
    "    This wrapper permits to create a gymnasium env where the action space is the cartesian product of agents' action space\n",
    "    and the observation space is the cartesian product of agents' observation space.\n",
    "\n",
    "    This permits to train a single \"super-agent\" which will receive all observations and distribute actions that has to be made by all \"sub-agents\".\n",
    "\n",
    "    WARNING : USE THIS WRAPPER ONLY IF YOU FOLLOW ALL ASSUMPTIONS BELOW !!!\n",
    "     - The env is a pettingzoo env with the parallel API (ParallelEnv) (can be wrapped)\n",
    "     - All agents of the env has a Box action space and a Box observation space\n",
    "     - All agents of the env must have the same bounds, dtypes and a shape of (1,) for the Box representing their action space\n",
    "     - All agents of the env must have the same bounds, dtypes and a 1D shape of same size for the Box space representing their observation space\n",
    "\n",
    "\n",
    "    Do not forget to use env.reset() before creating SingleAgentWrapperEnv(env) to properly initialize all attributes of the env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, options = None) :\n",
    "        super(SingleAgentWrapperEnv, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.agents = self.env.unwrapped.agents\n",
    "        self.nb_agent = len(self.agents)\n",
    "\n",
    "\n",
    "        # Creating observation_space\n",
    "        temp_space = self.env.observation_space(self.agents[0])\n",
    "        low_bound = min(temp_space.low) # Not optimized this using min instead of taking the lowest value for each element grows the observation_space\n",
    "        high_bound = max(temp_space.high) # Same here with the max\n",
    "        dtype = temp_space.dtype\n",
    "\n",
    "        shape = (self.nb_agent * temp_space.shape[0],)\n",
    "        self.observation_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "        \n",
    "        # Creating action_space\n",
    "        temp_space = self.env.action_space(self.agents[0])\n",
    "        low_bound = temp_space.low # Lowest value an action of an agent can take\n",
    "        high_bound = temp_space.high # Highest value an action of an agent can take\n",
    "        dtype = temp_space.dtype # The exact type of an action\n",
    "        \n",
    "        shape = (self.nb_agent,) # A vector container at index i the action made by self.env.agents[i]\n",
    "        self.action_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        observations, infos = self.env.reset(seed, options)\n",
    "\n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "        return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        The action in input is a vector containing actions of each agents\n",
    "        \"\"\"\n",
    "        dict_actions = {self.agents[i]: np.array(object = [action[i]], dtype=np.float32) for i in range(self.nb_agent)}\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(dict_actions)\n",
    "\n",
    "        \n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        termination = False\n",
    "        truncation = False\n",
    "        for agent in self.agents :\n",
    "            # Reward is the mean of all agents' reward\n",
    "            reward += rewards[agent]\n",
    "            reward = reward / self.nb_agent\n",
    "\n",
    "            # Episode ends as soon as it ends for one agent\n",
    "            if terminations[agent] :\n",
    "                termination = True\n",
    "\n",
    "            if truncations[agent] :\n",
    "                truncation = True\n",
    "\n",
    "        return obs, reward, termination or len(self.agents) != self.nb_agent, truncation, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afefbc-8d86-4a52-a238-f607ea6b5fbe",
   "metadata": {},
   "source": [
    "### Use Grey Scale Image as observation and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da4aa78-624a-4600-a528-a79c746e3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "from supersuit import color_reduction_v0\n",
    "from supersuit import resize_v1\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "grey_scale_env = color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = resize_v1(grey_scale_env, 114, 30) # Divide by 4 quality of each image observed by each agent\n",
    "grey_scale_env = flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0ac2-c3d4-40f2-85fd-fec266b2e2aa",
   "metadata": {},
   "source": [
    "### Testing the wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adca4965-c10d-4d31-b8d8-6ee13604cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (51300,), uint8)\n",
      "Test of the Wrapped Environment is done.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import supersuit\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "check_env(wrappedEnv)\n",
    "\n",
    "while env.aec_env.agents :\n",
    "    # this is where you would insert your policy\n",
    "    actions = [env.action_space(env.agents[0]).sample()[0] for agent in env.aec_env.agents]\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = wrappedEnv.step(actions)\n",
    "\n",
    "env.close()\n",
    "print(wrappedEnv.observation_space)\n",
    "print(\"Test of the Wrapped Environment is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547fc2-a233-444a-a899-b9485797753f",
   "metadata": {},
   "source": [
    "### Initializing PPO agent on wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85161c0a-7673-41db-91ea-b484a996f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(n_pistons = 10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "\n",
    "# Initializing PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrappedEnv)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e15778-95bf-4bdc-b8e6-929b7220eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e733e4d-b547-44f0-b7e3-31ce63ecd1ec",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc05732-b267-4dfe-a624-33ccd600a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bd1e1-799e-4964-8302-2a1f4bafa92d",
   "metadata": {},
   "source": [
    "### Save PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb09a4b-b822-453f-8bc8-430c0057a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df32c-f968-46f5-be85-241ccef75ae3",
   "metadata": {},
   "source": [
    "### Import PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f4d8d3-1bf0-4eaa-b8ff-ea4846d195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413040ca-0192-46b3-9657-d237ca6f0ecb",
   "metadata": {},
   "source": [
    "### Test PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35097e7c-63e7-4653-9d3b-f095e24bdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons=10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "vec_env = make_vec_env(lambda : wrappedEnv, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2466-994d-4c0c-bd55-abda258edf98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7062-5d6a-43b0-ba3e-42eaddc1f60b",
   "metadata": {},
   "source": [
    "All pistons have the same goal, same rewards and same observations.\n",
    "\n",
    "Instead of training each pistons or training a single agent controlling all pistons, we could consider all pistons being the same one.\n",
    "\n",
    "In this case we would use all observations and rewards from all pistons to train only a single piston.\n",
    "And then treat all pistons as being a copy of this single piston.\n",
    "\n",
    "\n",
    "In fact parameter sharing cannot work here based on how rewards are distributed in this environment : same reward for each agent. Thus useless pistons (and actions they made) will have the same impact (even a bigger impact since they are more than usefull ones) as usefull pistons which make the ball going to the left wall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c271-c22c-4f83-84c0-7cb420981466",
   "metadata": {},
   "source": [
    "### Initialize PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56591909-3792-4d6d-aa44-38906707b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (30, 114, 5), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import numpy as np\n",
    "import supersuit\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode = None, n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "#model = PPO(\"CnnPolicy\", env)\n",
    "model = PPO(\"MlpPolicy\", env)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c549bc5a-0e26-436e-845a-fcbaefdc8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc67a6d-b332-4a0d-92ad-7093ab3adab2",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b756427-c7de-4992-a206-57b750cb7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f42dd-9792-42e9-ade0-320071a30851",
   "metadata": {},
   "source": [
    "### Test PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66859-9e1d-41ee-b130-c09f36649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "import supersuit\n",
    "\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode = \"human\", n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "\n",
    "#vec_env = make_vec_env(lambda : env, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = np.array([False])\n",
    "while not done.any() :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3255a-0d86-4d03-b121-02a3d5bf83a7",
   "metadata": {},
   "source": [
    "# RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bb471-b656-4dbf-aa23-657b7ae8e71c",
   "metadata": {},
   "source": [
    "### Register Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f651d9b-ab7f-4514-bd78-d40fd4c44bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 16:55:55,258\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m /home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 2025-12-12 16:56:19,906\tERROR multi_agent_env_runner.py:855 -- Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m In particular, the `reset()` method seems to be faulty.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `from gym.spaces import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    setting).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/pre_checks/env.py\", line 46, in check_multiagent_environments\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     obs_and_infos = env.reset(seed=42, options={})\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 213, in reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     obs, info = self.par_env.reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     ^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m TypeError: cannot unpack non-iterable NoneType object\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 853, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     check_multiagent_environments(env.unwrapped)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/pre_checks/env.py\", line 48, in check_multiagent_environments\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m ValueError: Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m In particular, the `reset()` method seems to be faulty.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `from gym.spaces import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    setting).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m 2025-12-12 16:56:21,032\tERROR actor_manager.py:186 -- Worker exception caught during `apply()`: cannot unpack non-iterable NoneType object\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 182, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 276, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 564, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     observations, infos = self._try_env_reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 248, in _try_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     raise e\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     self._observations[i], self._infos[i] = env.reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m                                             ^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m     return self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-12 16:56:22,756 E 95758 95758] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-12-12 16:56:25,191 E 95842 95842] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m /home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 2025-12-12 16:56:19,954\tERROR multi_agent_env_runner.py:855 -- Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m In particular, the `reset()` method seems to be faulty.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m Learn more about the most important changes here:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m In order to fix this problem, do the following:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 1) Run `pip install gymnasium` on your command line.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 2) Change all your import statements in your code from\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    `from gym.spaces import Discrete` -> `from gymnasium.spaces import Discrete`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m For your custom (single agent) gym.Env classes:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m      EnvCompatibility` wrapper class.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 4.2) Alternatively to 4.1:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    seed=None, options=None)'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    method.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Change your `reset()` method to have the call signature\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    'def reset(self, *, seed=None, options=None)'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    `reset()` method.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    setting).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    per-agent dict).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/pre_checks/env.py\", line 48, in check_multiagent_environments\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     obs_and_infos = env.reset(seed=42, options={})\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 213, in reset\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     obs, info = self.par_env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     ^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m TypeError: cannot unpack non-iterable NoneType object\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 853, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     check_multiagent_environments(env.unwrapped)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m ValueError: Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95900)\u001b[0m [2025-12-12 16:56:28,463 E 95900 95955] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m 2025-12-12 16:56:21,050\tERROR actor_manager.py:186 -- Worker exception caught during `apply()`: cannot unpack non-iterable NoneType object\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 182, in apply\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 222, in sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 276, in _sample\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 564, in _reset_envs\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     observations, infos = self._try_env_reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 232, in _try_env_reset\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     raise e\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     self._observations[i], self._infos[i] = env.reset(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m                                             ^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95897)\u001b[0m     return self.env.reset(seed=seed, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 00000000c88bc639e58c24e80025e6b1954369df1ce4548ffc14a27a585d4083 Worker ID: ea40a3baa2383065ac6b5b7c705682c6b078822d036cd7371d8a9a3d Node ID: fa72a426818fb64be56d548a3101fda9e34e3f8765cbfb934af04e40 Worker IP address: 134.206.154.215 Worker port: 43595 Worker PID: 95900 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m /home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 2025-12-12 16:57:23,907\tERROR multi_agent_env_runner.py:855 -- Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m In particular, the `reset()` method seems to be faulty.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `from gym.spaces import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    setting).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/pre_checks/env.py\", line 46, in check_multiagent_environments\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m     obs_and_infos = env.reset(seed=42, options={})\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py\", line 213, in reset\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m     obs, info = self.par_env.reset(seed=seed, options=options)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m     ^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m TypeError: cannot unpack non-iterable NoneType object\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 853, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m     check_multiagent_environments(env.unwrapped)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/utils/pre_checks/env.py\", line 48, in check_multiagent_environments\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m ValueError: Your environment (<ParallelPettingZooEnv<rllib-multi-agent-env-v0>>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m In particular, the `reset()` method seems to be faulty.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `from gym.spaces import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    setting).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95893)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=95896)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "import supersuit\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "env_name = \"pistonball_v6\"\n",
    "\n",
    "def env_creator(args) :\n",
    "    env = pistonball_v6.parallel_env()\n",
    "    env = supersuit.color_reduction_v0(env, mode='full') # image to black and white\n",
    "    env = supersuit.resize_v1(env, 84, 84) # Reduce size of image\n",
    "    env = supersuit.reshape_v0(env, env.observation_space('piston_0').shape + (1,))\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb53fb1-50d9-44c1-815e-1c116304a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(\"\")\n",
    "env.reset()\n",
    "print(env.observation_space('piston_0').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a31e-2f51-478a-bd3a-aec522c3ceef",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acf3c0-5a0d-4b06-9701-8a5b741ddbb6",
   "metadata": {},
   "source": [
    "One policy per agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f972b88-298c-48bd-9300-0335c705cb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    .rl_module(\\n        rl_module_spec = MultiRLModuleSpec(rl_module_specs={\\n            agent_id : RLModuleSpec() for agent_id in env.unwrapped.agents\\n        })\\n    )\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=pistonball_v6.parallel_env)\n",
    "    .multi_agent(\n",
    "        policies = {elem for elem in env.unwrapped.agents}, \n",
    "        policy_mapping_fn = lambda agent_id, episode, **kwargs : agent_id\n",
    "    )\n",
    ")\n",
    "\n",
    "config.framework(\"torch\")\n",
    "config.validate()\n",
    "print(config.is_multi_agent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    .rl_module(\n",
    "        rl_module_spec = MultiRLModuleSpec(rl_module_specs={\n",
    "            agent_id : RLModuleSpec() for agent_id in env.unwrapped.agents\n",
    "        })\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d1c8-36a7-49ce-a766-81245844af48",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c9874-5710-4c3e-8559-cab9aa204993",
   "metadata": {},
   "source": [
    "With Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee482727-65ec-429a-8c92-ee5ac0f2407d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-12-12 16:17:22</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:11.46        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.2/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pistonball_v6_a1b00_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-12-12_16-17-05_089833_74177/artifacts/2025-12-12_16-17-11/PPO_2025-12-12_16-17-05/driver_artifacts/PPO_pistonball_v6_a1b00_00000_0_2025-12-12_16-17-11/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pistonball_v6_a1b00_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 16:17:11,272\tWARNING algorithm_config.py:4941 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation doesn't occur automatically with each call to `Algorithm.train()`. Instead, you have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "2025-12-12 16:17:11,274\tWARNING algorithm_config.py:4941 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation doesn't occur automatically with each call to `Algorithm.train()`. Instead, you have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:16,665\tWARNING algorithm_config.py:4941 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation doesn't occur automatically with each call to `Algorithm.train()`. Instead, you have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:16,665\tWARNING algorithm_config.py:5053 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m [2025-12-12 16:17:17,093 E 92554 92554] core_worker.cc:2200: Actor with class name: 'MultiAgentEnvRunner' and ID: '7e1c5e350b1caf7af32fa84501000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92621, ip=134.206.154.215, actor_id=7e1c5e350b1caf7af32fa84501000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7b7c37a234d0>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/gymnasium/envs/registration.py\", line 689, in make\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m                ^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/gymnasium/envs/registration.py\", line 533, in _find_spec\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/gymnasium/envs/registration.py\", line 399, in _check_version_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/gymnasium/envs/registration.py\", line 376, in _check_name_exists\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m gymnasium.error.NameNotFound: Environment `pistonball_v6` doesn't exist.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92621, ip=134.206.154.215, actor_id=7e1c5e350b1caf7af32fa84501000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7b7c37a234d0>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     self.make_env()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 836, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     self.env = make_vec(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m                ^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     env = SyncVectorMultiAgentEnv(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 29, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     self.envs = [env_fn() for env_fn in self.env_fns]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m                  ^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/registration.py\", line 62, in create_single_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     single_env = gym.make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/gymnasium/envs/registration.py\", line 742, in make\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/utils/__init__.py\", line 113, in _gym_env_creator\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('pistonball_v6') is:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m Try one of the following:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m Exception ignored in: <function VectorMultiAgentEnv.__del__ at 0x7b7c37a1c7c0>\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/vector_multi_agent_env.py\", line 80, in __del__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     self.close()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/vector_multi_agent_env.py\", line 66, in close\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     self.close_extras(**kwargs)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 195, in close_extras\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m     [env.close() for env in self.envs]\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92621)\u001b[0m TypeError: 'NoneType' object is not iterable\n",
      "2025-12-12 16:17:22,715\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_pistonball_v6_a1b00_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=92554, ip=134.206.154.215, actor_id=dc05691216cf4539d82a918d01000000, repr=PPO(env=pistonball_v6; env-runners=2; learners=0; multi-agent=True))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py\", line 578, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py\", line 824, in setup\n",
      "    self.env_runner_group = EnvRunnerGroup(\n",
      "                            ^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py\", line 196, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py\", line 284, in _setup\n",
      "    spaces = self.get_spaces()\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py\", line 312, in get_spaces\n",
      "    spaces = self.foreach_env_runner(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: list index out of range\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:22,698\tERROR actor_manager.py:973 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92621, ip=134.206.154.215, actor_id=7e1c5e350b1caf7af32fa84501000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7b7c37a234d0>)\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 1 out of service.\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:22,698\tERROR actor_manager.py:973 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92622, ip=134.206.154.215, actor_id=1cd741a44c02edc021f57d5e01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x793681b63e90>)\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 2 out of service.\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:22,698\tERROR env_runner_group.py:793 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92621, ip=134.206.154.215, actor_id=7e1c5e350b1caf7af32fa84501000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7b7c37a234d0>)\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m 2025-12-12 16:17:22,698\tERROR env_runner_group.py:793 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=92622, ip=134.206.154.215, actor_id=1cd741a44c02edc021f57d5e01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x793681b63e90>)\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m \n",
      "\u001b[36m(PPO pid=92554)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=92554, ip=134.206.154.215, actor_id=dc05691216cf4539d82a918d01000000, repr=PPO(env=pistonball_v6; env-runners=2; learners=0; multi-agent=True))\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     super().__init__(\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm.py\", line 824, in setup\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     self.env_runner_group = EnvRunnerGroup(\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m                             ^^^^^^^^^^^^^^^\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     self._setup(\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py\", line 284, in _setup\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     spaces = self.get_spaces()\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m              ^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/env_runner_group.py\", line 312, in get_spaces\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m     spaces = self.foreach_env_runner(\n",
      "\u001b[36m(PPO pid=92554)\u001b[0m IndexError: list index out of range\n",
      "\u001b[36m(MultiAgentEnvRunner pid=92622)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=92622)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=92622)\u001b[0m \n",
      "2025-12-12 16:17:22,725\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/enzo/ray_results/PPO_2025-12-12_16-17-05' in 0.0036s.\n",
      "2025-12-12 16:17:22,727\tERROR tune.py:1037 -- Trials did not complete: [PPO_pistonball_v6_a1b00_00000]\n",
      "2025-12-12 16:17:22,728\tINFO tune.py:1041 -- Total run time: 11.48 seconds (11.46 seconds for the tuning loop).\n",
      "2025-12-12 16:17:22,730\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- PPO_pistonball_v6_a1b00_00000: FileNotFoundError('Could not fetch metrics for PPO_pistonball_v6_a1b00_00000: both result.json and progress.csv were not found at /home/enzo/ray_results/PPO_2025-12-12_16-17-05/PPO_pistonball_v6_a1b00_00000_0_2025-12-12_16-17-11')\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-12 16:17:35,329 E 92026 92026] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space = config,\n",
    ")\n",
    "\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5e432-0603-4d3f-8ad7-6403b83e6aaf",
   "metadata": {},
   "source": [
    "Without Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd30e42b-714f-4f44-8877-36575516e873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 16:56:21,019\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "[2025-12-12 16:56:28,856 E 74177 95892] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-12-12 16:57:21,037\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-12-12 16:57:22,872\tERROR actor_manager.py:973 -- Ray error (The actor 359aa17652c39a73e7f38a0a01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-12-12 16:57:22,873\tERROR actor_manager.py:973 -- Ray error (The actor 8950e7b3ab1d209b48849ce301000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-12-12 16:57:22,873\tERROR actor_manager.py:771 -- The actor 359aa17652c39a73e7f38a0a01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-12-12 16:57:22,874\tERROR actor_manager.py:771 -- The actor 8950e7b3ab1d209b48849ce301000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timers': {'training_iteration': 60.01145464298315,\n",
       "  'restore_env_runners': 1.7073936760425568e-05,\n",
       "  'training_step': 60.01118877506815,\n",
       "  'env_runner_sampling_timer': 60.011092095053755},\n",
       " 'num_training_step_calls_per_iteration': 1,\n",
       " 'num_env_steps_sampled_lifetime': 0,\n",
       " 'fault_tolerance': {'num_healthy_workers': 0,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-12-12_16-57-22',\n",
       " 'timestamp': 1765555042,\n",
       " 'time_this_iter_s': 61.84901475906372,\n",
       " 'time_total_s': 61.84901475906372,\n",
       " 'pid': 74177,\n",
       " 'hostname': 'smac-Precision-Tower-3620',\n",
       " 'node_ip': '134.206.154.215',\n",
       " 'config': {'exploration_config': {},\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'torch_ddp_kwargs': {},\n",
       "  'torch_skip_nan_gradients': False,\n",
       "  'env': 'pistonball_v6',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'disable_env_checking': False,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'create_local_env_runner': True,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'gym_env_vectorize_mode': 'SYNC',\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'episodes_to_numpy': True,\n",
       "  'max_requests_in_flight_per_env_runner': 1,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'merge_env_runner_states': 'training_only',\n",
       "  'broadcast_env_runner_states': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  '_is_online': True,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 'auto',\n",
       "  'num_aggregator_actors_per_learner': 0,\n",
       "  'max_requests_in_flight_per_aggregator_actor': 3,\n",
       "  'local_gpu_idx': 0,\n",
       "  'max_requests_in_flight_per_learner': 3,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  '_train_batch_size_per_learner': None,\n",
       "  'train_batch_size': 4000,\n",
       "  'num_epochs': 30,\n",
       "  'minibatch_size': 128,\n",
       "  'shuffle_batch_per_epoch': True,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'log_std_clip_param': 20.0,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  '_learner_class': None,\n",
       "  'callbacks_on_algorithm_init': None,\n",
       "  'callbacks_on_env_runners_recreated': None,\n",
       "  'callbacks_on_offline_eval_runners_recreated': None,\n",
       "  'callbacks_on_checkpoint_loaded': None,\n",
       "  'callbacks_on_environment_created': None,\n",
       "  'callbacks_on_episode_created': None,\n",
       "  'callbacks_on_episode_start': None,\n",
       "  'callbacks_on_episode_step': None,\n",
       "  'callbacks_on_episode_end': None,\n",
       "  'callbacks_on_evaluate_start': None,\n",
       "  'callbacks_on_evaluate_end': None,\n",
       "  'callbacks_on_evaluate_offline_start': None,\n",
       "  'callbacks_on_evaluate_offline_end': None,\n",
       "  'callbacks_on_sample_end': None,\n",
       "  'callbacks_on_train_result': None,\n",
       "  'explore': True,\n",
       "  'enable_rl_module_and_learner': True,\n",
       "  'enable_env_runner_and_connector_v2': True,\n",
       "  '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'offline_data_class': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_read_sample_batches': False,\n",
       "  'input_read_batch_size': None,\n",
       "  'input_filesystem': None,\n",
       "  'input_filesystem_kwargs': {},\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'materialize_data': False,\n",
       "  'materialize_mapped_data': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'ignore_final_observation': False,\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_buffer_class': None,\n",
       "  'prelearner_buffer_kwargs': {},\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_remaining_data': False,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
       "  'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'offline_evaluation_interval': None,\n",
       "  'num_offline_eval_runners': 0,\n",
       "  'offline_evaluation_type': None,\n",
       "  'offline_eval_runner_class': None,\n",
       "  'offline_loss_for_module_fn': None,\n",
       "  'offline_evaluation_duration': 1,\n",
       "  'offline_evaluation_parallel_to_training': False,\n",
       "  'offline_evaluation_timeout_s': 120.0,\n",
       "  'num_cpus_per_offline_eval_runner': 1,\n",
       "  'num_gpus_per_offline_eval_runner': 0,\n",
       "  'custom_resources_per_offline_eval_runner': {},\n",
       "  'restart_failed_offline_eval_runners': True,\n",
       "  'ignore_offline_eval_runner_failures': False,\n",
       "  'max_num_offline_eval_runner_restarts': 1000,\n",
       "  'offline_eval_runner_restore_timeout_s': 1800.0,\n",
       "  'max_requests_in_flight_per_offline_eval_runner': 1,\n",
       "  'validate_offline_eval_runners_after_construction': True,\n",
       "  'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
       "  'offline_eval_rl_module_inference_only': False,\n",
       "  'broadcast_offline_eval_runner_states': False,\n",
       "  'offline_eval_batch_size_per_runner': 256,\n",
       "  'dataset_num_iters_per_eval_runner': 1,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'log_gradients': False,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'restart_failed_env_runners': True,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30.0,\n",
       "  'env_runner_restore_timeout_s': 1800.0,\n",
       "  '_model_config': {},\n",
       "  '_rl_module_spec': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_validate_config': True,\n",
       "  '_use_msgpack_checkpoints': False,\n",
       "  '_torch_grad_scaler_class': None,\n",
       "  '_torch_lr_scheduler_classes': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'env_task_fn': -1,\n",
       "  'enable_connectors': -1,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'lr_schedule': None,\n",
       "  'sgd_minibatch_size': -1,\n",
       "  'vf_share_layers': -1,\n",
       "  'class': ray.rllib.algorithms.ppo.ppo.PPOConfig,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'piston_9': (None, None, None, None),\n",
       "   'piston_1': (None, None, None, None),\n",
       "   'piston_3': (None, None, None, None),\n",
       "   'piston_5': (None, None, None, None),\n",
       "   'piston_14': (None, None, None, None),\n",
       "   'piston_12': (None, None, None, None),\n",
       "   'piston_15': (None, None, None, None),\n",
       "   'piston_6': (None, None, None, None),\n",
       "   'piston_7': (None, None, None, None),\n",
       "   'piston_4': (None, None, None, None),\n",
       "   'piston_11': (None, None, None, None),\n",
       "   'piston_2': (None, None, None, None),\n",
       "   'piston_10': (None, None, None, None),\n",
       "   'piston_0': (None, None, None, None),\n",
       "   'piston_16': (None, None, None, None),\n",
       "   'piston_18': (None, None, None, None),\n",
       "   'piston_19': (None, None, None, None),\n",
       "   'piston_8': (None, None, None, None),\n",
       "   'piston_13': (None, None, None, None),\n",
       "   'piston_17': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.callbacks.callbacks.RLlibCallback,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 61.84901475906372,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(13.11011235955056),\n",
       "  'ram_util_percent': np.float64(60.30337078651685)}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = config.build_algo()\n",
    "\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28716cc8-6643-4a32-8659-867b4cc31f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 16:20:40,811\tERROR actor_manager.py:973 -- Ray error (The actor 674a12fded5e55e8a65cbefe01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or may not have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-12-12 16:20:40,812\tERROR actor_manager.py:771 -- The actor 674a12fded5e55e8a65cbefe01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-12-12 16:20:40,813\tWARNING algorithm.py:2074 -- Calling `sample()` on your remote evaluation worker(s) resulted in all workers crashing! Make sure a) your environment is not too unstable, b) you have enough evaluation workers (`config.evaluation(evaluation_num_env_runners=...)`) to cover for occasional losses, and c) you use the `config.fault_tolerance(restart_failed_env_runners=True)` setting.\n",
      "2025-12-12 16:20:40,813\tWARNING algorithm.py:2105 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 10 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.\n",
      "2025-12-12 16:20:40,814\tWARNING algorithm.py:1516 -- No evaluation results found for this iteration. This can happen if the evaluation worker(s) is/are not healthy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f96dfec0-756a-4421-ae35-9637ba1b6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.stop()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249016e5-bd39-4ac2-b4ec-0d2bc3b1d345",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86a324db-e713-4c33-a792-33d8b6050512",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgym\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(config.observation_space)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(config.get_multi_agent_setup(env = gym.make(\u001b[33m'\u001b[39m\u001b[33mpistonball_v6\u001b[39m\u001b[33m'\u001b[39m)))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "print(config.observation_space)\n",
    "print(config.get_multi_agent_setup(env = gym.make('pistonball_v6')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65632ba-7c09-484b-bcde-d2f726996b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
