{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71071e0-387b-4ce4-8308-9f3d6ee9353a",
   "metadata": {},
   "source": [
    "# Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac2f7f-7b83-427c-9f8c-750d64a0a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "/media/thewalder/MegaDisk/anaconda3/envs/MARL/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# Creates env\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    # Get observation and reward of the agent\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Randomly selected action from the action space\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eed9e-b28f-490b-8444-1573a5788000",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab570e88-c1f9-47df-a477-7f7f662d7e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons = 20)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a308f2-f19e-4d57-8227-9835c61fe16b",
   "metadata": {},
   "source": [
    "# Wrapping Environment for Fully Centralized Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f722ee79-b703-42ee-8b2d-29b13b67aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SingleAgentWrapperEnv(gymnasium.Env) :\n",
    "    \"\"\"\n",
    "    This wrapper permits to create a gymnasium env where the action space is the cartesian product of agents' action space\n",
    "    and the observation space is the cartesian product of agents' observation space.\n",
    "\n",
    "    This permits to train a single \"super-agent\" which will receive all observations and distribute actions that has to be made by all \"sub-agents\".\n",
    "\n",
    "    WARNING : USE THIS WRAPPER ONLY IF YOU FOLLOW ALL ASSUMPTIONS BELOW !!!\n",
    "     - The env is a pettingzoo env with the parallel API (ParallelEnv) (can be wrapped)\n",
    "     - All agents of the env has a Box action space and a Box observation space\n",
    "     - All agents of the env must have the same bounds, dtypes and a shape of (1,) for the Box representing their action space\n",
    "     - All agents of the env must have the same bounds, dtypes and a 1D shape of same size for the Box space representing their observation space\n",
    "\n",
    "\n",
    "    Do not forget to use env.reset() before creating SingleAgentWrapperEnv(env) to properly initialize all attributes of the env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, options = None) :\n",
    "        super(SingleAgentWrapperEnv, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.agents = self.env.unwrapped.agents\n",
    "        self.nb_agent = len(self.agents)\n",
    "\n",
    "\n",
    "        # Creating observation_space\n",
    "        temp_space = self.env.observation_space(self.agents[0])\n",
    "        low_bound = min(temp_space.low) # Not optimized this using min instead of taking the lowest value for each element grows the observation_space\n",
    "        high_bound = max(temp_space.high) # Same here with the max\n",
    "        dtype = temp_space.dtype\n",
    "\n",
    "        shape = (self.nb_agent * temp_space.shape[0],)\n",
    "        self.observation_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "        \n",
    "        # Creating action_space\n",
    "        temp_space = self.env.action_space(self.agents[0])\n",
    "        low_bound = temp_space.low # Lowest value an action of an agent can take\n",
    "        high_bound = temp_space.high # Highest value an action of an agent can take\n",
    "        dtype = temp_space.dtype # The exact type of an action\n",
    "        \n",
    "        shape = (self.nb_agent,) # A vector container at index i the action made by self.env.agents[i]\n",
    "        self.action_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        observations, infos = self.env.reset(seed, options)\n",
    "\n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "        return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        The action in input is a vector containing actions of each agents\n",
    "        \"\"\"\n",
    "        dict_actions = {self.agents[i]: np.array(object = [action[i]], dtype=np.float32) for i in range(self.nb_agent)}\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(dict_actions)\n",
    "\n",
    "        \n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        termination = False\n",
    "        truncation = False\n",
    "        for agent in self.agents :\n",
    "            # Reward is the mean of all agents' reward\n",
    "            reward += rewards[agent]\n",
    "            reward = reward / self.nb_agent\n",
    "\n",
    "            # Episode ends as soon as it ends for one agent\n",
    "            if terminations[agent] :\n",
    "                termination = True\n",
    "\n",
    "            if truncations[agent] :\n",
    "                truncation = True\n",
    "\n",
    "        return obs, reward, termination or len(self.agents) != self.nb_agent, truncation, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afefbc-8d86-4a52-a238-f607ea6b5fbe",
   "metadata": {},
   "source": [
    "### Use Grey Scale Image as observation and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da4aa78-624a-4600-a528-a79c746e3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "from supersuit import color_reduction_v0\n",
    "from supersuit import resize_v1\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "grey_scale_env = color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = resize_v1(grey_scale_env, 114, 30) # Divide by 4 quality of each image observed by each agent\n",
    "grey_scale_env = flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0ac2-c3d4-40f2-85fd-fec266b2e2aa",
   "metadata": {},
   "source": [
    "### Testing the wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adca4965-c10d-4d31-b8d8-6ee13604cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (85500,), uint8)\n",
      "Test of the Wrapped Environment is done.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import supersuit\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "check_env(wrappedEnv)\n",
    "\n",
    "while env.aec_env.agents :\n",
    "    # this is where you would insert your policy\n",
    "    actions = [env.action_space(env.agents[0]).sample()[0] for agent in env.aec_env.agents]\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = wrappedEnv.step(actions)\n",
    "\n",
    "env.close()\n",
    "print(wrappedEnv.observation_space)\n",
    "print(\"Test of the Wrapped Environment is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547fc2-a233-444a-a899-b9485797753f",
   "metadata": {},
   "source": [
    "### Initializing PPO agent on wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85161c0a-7673-41db-91ea-b484a996f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(n_pistons = 10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "\n",
    "# Initializing PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrappedEnv)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e15778-95bf-4bdc-b8e6-929b7220eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=25650, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=25650, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e733e4d-b547-44f0-b7e3-31ce63ecd1ec",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc05732-b267-4dfe-a624-33ccd600a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=30000) # Took 1 hour for 30000 steps without reducing image size for 10 pistons\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413040ca-0192-46b3-9657-d237ca6f0ecb",
   "metadata": {},
   "source": [
    "### Test PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35097e7c-63e7-4653-9d3b-f095e24bdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons=10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "vec_env = make_vec_env(lambda : wrappedEnv, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2466-994d-4c0c-bd55-abda258edf98",
   "metadata": {},
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7062-5d6a-43b0-ba3e-42eaddc1f60b",
   "metadata": {},
   "source": [
    "All pistons have the same goal, same rewards and same observations.\n",
    "\n",
    "Instead of training each pistons or training a single agent controlling all pistons, we could consider all pistons being the same one.\n",
    "\n",
    "In this case we would use all observations and rewards from all pistons to train only a single piston.\n",
    "And then treat all pistons as being a copy of this single piston.\n",
    "\n",
    "\n",
    "In fact parameter sharing cannot work here based on how rewards are distributed in this environment : same reward for each agent. Thus useless pistons (and actions they made) will have the same impact (even a bigger impact since they are more than usefull ones) as usefull pistons which make the ball going to the left wall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c271-c22c-4f83-84c0-7cb420981466",
   "metadata": {},
   "source": [
    "### Initialize PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56591909-3792-4d6d-aa44-38906707b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (30, 114, 5), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import numpy as np\n",
    "import supersuit\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode = None, n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "#model = PPO(\"CnnPolicy\", env)\n",
    "model = PPO(\"MlpPolicy\", env)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c549bc5a-0e26-436e-845a-fcbaefdc8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc67a6d-b332-4a0d-92ad-7093ab3adab2",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b756427-c7de-4992-a206-57b750cb7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f42dd-9792-42e9-ade0-320071a30851",
   "metadata": {},
   "source": [
    "### Test PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66859-9e1d-41ee-b130-c09f36649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "import supersuit\n",
    "\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode = \"human\", n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "\n",
    "#vec_env = make_vec_env(lambda : env, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = np.array([False])\n",
    "while not done.any() :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777553b-1501-488a-b0fe-be138eee5474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
