{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71071e0-387b-4ce4-8308-9f3d6ee9353a",
   "metadata": {},
   "source": [
    "# Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ac2f7f-7b83-427c-9f8c-750d64a0a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# Creates env\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    # Get observation and reward of the agent\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # Randomly selected action from the action space\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8eed9e-b28f-490b-8444-1573a5788000",
   "metadata": {},
   "source": [
    "# Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab570e88-c1f9-47df-a477-7f7f662d7e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons = 20)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a308f2-f19e-4d57-8227-9835c61fe16b",
   "metadata": {},
   "source": [
    "# Wrapping Environment for Fully Centralized Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f722ee79-b703-42ee-8b2d-29b13b67aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class SingleAgentWrapperEnv(gymnasium.Env) :\n",
    "    \"\"\"\n",
    "    This wrapper permits to create a gymnasium env where the action space is the cartesian product of agents' action space\n",
    "    and the observation space is the cartesian product of agents' observation space.\n",
    "\n",
    "    This permits to train a single \"super-agent\" which will receive all observations and distribute actions that has to be made by all \"sub-agents\".\n",
    "\n",
    "    WARNING : USE THIS WRAPPER ONLY IF YOU FOLLOW ALL ASSUMPTIONS BELOW !!!\n",
    "     - The env is a pettingzoo env with the parallel API (ParallelEnv) (can be wrapped)\n",
    "     - All agents of the env has a Box action space and a Box observation space\n",
    "     - All agents of the env must have the same bounds, dtypes and a shape of (1,) for the Box representing their action space\n",
    "     - All agents of the env must have the same bounds, dtypes and a 1D shape of same size for the Box space representing their observation space\n",
    "\n",
    "\n",
    "    Do not forget to use env.reset() before creating SingleAgentWrapperEnv(env) to properly initialize all attributes of the env\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, options = None) :\n",
    "        super(SingleAgentWrapperEnv, self).__init__()\n",
    "\n",
    "        self.env = env\n",
    "        self.agents = self.env.unwrapped.agents\n",
    "        self.nb_agent = len(self.agents)\n",
    "\n",
    "\n",
    "        # Creating observation_space\n",
    "        temp_space = self.env.observation_space(self.agents[0])\n",
    "        low_bound = min(temp_space.low) # Not optimized this using min instead of taking the lowest value for each element grows the observation_space\n",
    "        high_bound = max(temp_space.high) # Same here with the max\n",
    "        dtype = temp_space.dtype\n",
    "\n",
    "        shape = (self.nb_agent * temp_space.shape[0],)\n",
    "        self.observation_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "        \n",
    "        # Creating action_space\n",
    "        temp_space = self.env.action_space(self.agents[0])\n",
    "        low_bound = temp_space.low # Lowest value an action of an agent can take\n",
    "        high_bound = temp_space.high # Highest value an action of an agent can take\n",
    "        dtype = temp_space.dtype # The exact type of an action\n",
    "        \n",
    "        shape = (self.nb_agent,) # A vector container at index i the action made by self.env.agents[i]\n",
    "        self.action_space = spaces.Box(np.full(shape, low_bound), np.full(shape, high_bound), shape, dtype)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        observations, infos = self.env.reset(seed, options)\n",
    "\n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "        return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        The action in input is a vector containing actions of each agents\n",
    "        \"\"\"\n",
    "        dict_actions = {self.agents[i]: np.array(object = [action[i]], dtype=np.float32) for i in range(self.nb_agent)}\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = self.env.step(dict_actions)\n",
    "\n",
    "        \n",
    "        obs = np.array([], dtype = self.observation_space.dtype)\n",
    "        for i in range(self.nb_agent) :\n",
    "            obs = np.append(obs, observations[self.agents[i]]) # Concatenation of all observations\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        termination = False\n",
    "        truncation = False\n",
    "        for agent in self.agents :\n",
    "            # Reward is the mean of all agents' reward\n",
    "            reward += rewards[agent]\n",
    "            reward = reward / self.nb_agent\n",
    "\n",
    "            # Episode ends as soon as it ends for one agent\n",
    "            if terminations[agent] :\n",
    "                termination = True\n",
    "\n",
    "            if truncations[agent] :\n",
    "                truncation = True\n",
    "\n",
    "        return obs, reward, termination or len(self.agents) != self.nb_agent, truncation, infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afefbc-8d86-4a52-a238-f607ea6b5fbe",
   "metadata": {},
   "source": [
    "### Use Grey Scale Image as observation and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da4aa78-624a-4600-a528-a79c746e3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "from supersuit import color_reduction_v0\n",
    "from supersuit import resize_v1\n",
    "from supersuit import flatten_v0\n",
    "\n",
    "grey_scale_env = color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = resize_v1(grey_scale_env, 114, 30) # Divide by 4 quality of each image observed by each agent\n",
    "grey_scale_env = flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0ac2-c3d4-40f2-85fd-fec266b2e2aa",
   "metadata": {},
   "source": [
    "### Testing the wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adca4965-c10d-4d31-b8d8-6ee13604cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (51300,), uint8)\n",
      "Test of the Wrapped Environment is done.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import supersuit\n",
    "\n",
    "env = pistonball_v6.parallel_env()\n",
    "env.reset()\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "check_env(wrappedEnv)\n",
    "\n",
    "while env.aec_env.agents :\n",
    "    # this is where you would insert your policy\n",
    "    actions = [env.action_space(env.agents[0]).sample()[0] for agent in env.aec_env.agents]\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = wrappedEnv.step(actions)\n",
    "\n",
    "env.close()\n",
    "print(wrappedEnv.observation_space)\n",
    "print(\"Test of the Wrapped Environment is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7547fc2-a233-444a-a899-b9485797753f",
   "metadata": {},
   "source": [
    "### Initializing PPO agent on wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85161c0a-7673-41db-91ea-b484a996f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(n_pistons = 10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "\n",
    "# Initializing PPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrappedEnv)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e15778-95bf-4bdc-b8e6-929b7220eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=51300, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e733e4d-b547-44f0-b7e3-31ce63ecd1ec",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc05732-b267-4dfe-a624-33ccd600a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bd1e1-799e-4964-8302-2a1f4bafa92d",
   "metadata": {},
   "source": [
    "### Save PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb09a4b-b822-453f-8bc8-430c0057a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/thewalder/Big Disk/Travail/customEnv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df32c-f968-46f5-be85-241ccef75ae3",
   "metadata": {},
   "source": [
    "### Import PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f4d8d3-1bf0-4eaa-b8ff-ea4846d195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load(\"./models/crossProductPPO.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413040ca-0192-46b3-9657-d237ca6f0ecb",
   "metadata": {},
   "source": [
    "### Test PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35097e7c-63e7-4653-9d3b-f095e24bdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import supersuit\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\", n_pistons=10)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "grey_scale_env = supersuit.color_reduction_v0(env, mode='full') # This changes observations as grey scale images\n",
    "grey_scale_env = supersuit.resize_v1(grey_scale_env, 57, 15) # Divide by 8 size of each image observed by each agent\n",
    "grey_scale_env = supersuit.flatten_v0(grey_scale_env) # Flatten to 1D to make it compatible with our wrapper\n",
    "grey_scale_env = supersuit.frame_stack_v1(grey_scale_env, 3) # Observations are now the past 3 observations (so pistons can observe in which direction the ball moves)\n",
    "\n",
    "\n",
    "wrappedEnv = SingleAgentWrapperEnv(grey_scale_env)\n",
    "\n",
    "vec_env = make_vec_env(lambda : wrappedEnv, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2466-994d-4c0c-bd55-abda258edf98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b7062-5d6a-43b0-ba3e-42eaddc1f60b",
   "metadata": {},
   "source": [
    "All pistons have the same goal, same rewards and same observations.\n",
    "\n",
    "Instead of training each pistons or training a single agent controlling all pistons, we could consider all pistons being the same one.\n",
    "\n",
    "In this case we would use all observations and rewards from all pistons to train only a single piston.\n",
    "And then treat all pistons as being a copy of this single piston.\n",
    "\n",
    "\n",
    "In fact parameter sharing cannot work here based on how rewards are distributed in this environment : same reward for each agent. Thus useless pistons (and actions they made) will have the same impact (even a bigger impact since they are more than usefull ones) as usefull pistons which make the ball going to the left wall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c271-c22c-4f83-84c0-7cb420981466",
   "metadata": {},
   "source": [
    "### Initialize PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56591909-3792-4d6d-aa44-38906707b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (30, 114, 5), uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PPO model has been initialized.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import numpy as np\n",
    "import supersuit\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode = None, n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "#model = PPO(\"CnnPolicy\", env)\n",
    "model = PPO(\"MlpPolicy\", env)\n",
    "print(\"The PPO model has been initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c549bc5a-0e26-436e-845a-fcbaefdc8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks used by model.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=17100, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural networks used by model.\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc67a6d-b332-4a0d-92ad-7093ab3adab2",
   "metadata": {},
   "source": [
    "### Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b756427-c7de-4992-a206-57b750cb7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f42dd-9792-42e9-ade0-320071a30851",
   "metadata": {},
   "source": [
    "### Test PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f66859-9e1d-41ee-b130-c09f36649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "import supersuit\n",
    "\n",
    "\n",
    "# Creating env\n",
    "env = pistonball_v6.parallel_env(render_mode = \"human\", n_pistons=10) # We use AEC env\n",
    "env.reset()\n",
    "\n",
    "\n",
    "env = supersuit.color_reduction_v0(env, mode=\"full\")\n",
    "env = supersuit.resize_v1(env, 114, 30)\n",
    "env = supersuit.reshape_v0(env, env.observation_space(env.unwrapped.agents[0]).shape + (1,))\n",
    "env = supersuit.frame_stack_v1(env, 5)\n",
    "\n",
    "env = supersuit.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = supersuit.concat_vec_envs_v1(env, 1, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "\n",
    "#vec_env = make_vec_env(lambda : env, n_envs=1)\n",
    "\n",
    "\n",
    "# Get first observation\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = np.array([False])\n",
    "while not done.any() :\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3255a-0d86-4d03-b121-02a3d5bf83a7",
   "metadata": {},
   "source": [
    "# RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bb471-b656-4dbf-aa23-657b7ae8e71c",
   "metadata": {},
   "source": [
    "### Register Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f651d9b-ab7f-4514-bd78-d40fd4c44bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-11-14 16:46:40,020\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m /home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=448023, ip=134.206.154.215, actor_id=8b4c70a6015855c9030f8ba501000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7ed5c12b4740>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 122, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m     self.make_module()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 712, in make_module\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m     module_spec: RLModuleSpec = self.config.get_rl_module_spec(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m   File \"/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 4419, in get_rl_module_spec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=448023)\u001b[0m ValueError: When calling `AlgorithmConfig.get_rl_module_spec()`, the configuration must contain the `DEFAULT_MODULE_ID` key and all other keys' specs must have the setting `learner_only=True`! If you are using a more complex setup, call `AlgorithmConfig.get_multi_rl_module_spec(...)` instead.\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-14 16:47:08,081 E 447868 447868] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-14 16:47:09,955 E 447965 447965] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(pid=448019)\u001b[0m [2025-11-14 16:47:13,073 E 448019 448155] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "env_name = \"pistonball_v6\"\n",
    "\n",
    "def env_creator(args) :\n",
    "    env = pistonball_v6.parallel_env(n_pistons=20)\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb53fb1-50d9-44c1-815e-1c116304a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piston_0', 'piston_1', 'piston_2', 'piston_3', 'piston_4', 'piston_5', 'piston_6', 'piston_7', 'piston_8', 'piston_9', 'piston_10', 'piston_11', 'piston_12', 'piston_13', 'piston_14', 'piston_15', 'piston_16', 'piston_17', 'piston_18', 'piston_19']\n",
      "{'piston_0': 'a', 'piston_1': 'a', 'piston_2': 'a', 'piston_3': 'a', 'piston_4': 'a', 'piston_5': 'a', 'piston_6': 'a', 'piston_7': 'a', 'piston_8': 'a', 'piston_9': 'a', 'piston_10': 'a', 'piston_11': 'a', 'piston_12': 'a', 'piston_13': 'a', 'piston_14': 'a', 'piston_15': 'a', 'piston_16': 'a', 'piston_17': 'a', 'piston_18': 'a', 'piston_19': 'a'}\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(\"\")\n",
    "env.reset()\n",
    "print(env.unwrapped.agents)\n",
    "\n",
    "print({agent_id : \"a\" for agent_id in env.unwrapped.agents})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a31e-2f51-478a-bd3a-aec522c3ceef",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f972b88-298c-48bd-9300-0335c705cb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name)\n",
    "    .multi_agent(\n",
    "        policy_mapping_fn = lambda agent_id, episode, **kwargs : agent_id\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec = MultiRLModuleSpec(rl_module_specs={\n",
    "            agent_id : RLModuleSpec() for agent_id in env.unwrapped.agents\n",
    "        })\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(config.is_multi_agent)\n",
    "\n",
    "#algo = config.build_algo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d1c8-36a7-49ce-a766-81245844af48",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd30e42b-714f-4f44-8877-36575516e873",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43malgo\u001b[49m.train()\n\u001b[32m      3\u001b[39m algo.stop()\n\u001b[32m      4\u001b[39m ray.shutdown()\n",
      "\u001b[31mNameError\u001b[39m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "algo.train()\n",
    "\n",
    "algo.stop()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96dfec0-756a-4421-ae35-9637ba1b6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249016e5-bd39-4ac2-b4ec-0d2bc3b1d345",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd90dbcc-7c24-4e34-a321-865533c53e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-11-14 16:35:45,612\tINFO worker.py:2012 -- Started a local Ray instance.\n",
      "/home/enzo/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2025-11-14 16:35:49,137\tWARNING deprecation.py:50 -- DeprecationWarning: `config.training(num_sgd_iter=..)` has been deprecated. Use `config.training(num_epochs=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`policies` must be dict mapping PolicyID to PolicySpec OR a set/tuple/list of PolicyIDs!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m     80\u001b[39m register_env(env_name, _make_rllib_env)\n\u001b[32m     82\u001b[39m ModelCatalog.register_custom_model(\u001b[33m\"\u001b[39m\u001b[33mCNNModelV2\u001b[39m\u001b[33m\"\u001b[39m, CNNModelV2)\n\u001b[32m     84\u001b[39m config = (\n\u001b[32m     85\u001b[39m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_env_checking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gae\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_param\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mentropy_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvf_loss_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_sgd_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebugging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mERROR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRLLIB_NUM_GPUS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicies_to_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m )\n\u001b[32m    113\u001b[39m storage_uri = (Path(\u001b[33m\"\u001b[39m\u001b[33m~/ray_results\u001b[39m\u001b[33m\"\u001b[39m) / env_name).expanduser().resolve().as_uri()\n\u001b[32m    115\u001b[39m tune.run(\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPPO\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mPPO\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     config=config.to_dict(),\n\u001b[32m    122\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Depots/customEnv/lib/python3.12/site-packages/ray/rllib/algorithms/algorithm_config.py:3535\u001b[39m, in \u001b[36mAlgorithmConfig.multi_agent\u001b[39m\u001b[34m(self, policies, policy_map_capacity, policy_mapping_fn, policies_to_train, policy_states_are_swappable, observation_fn, count_steps_by, algorithm_config_overrides_per_module, replay_mode, policy_map_cache)\u001b[39m\n\u001b[32m   3533\u001b[39m         \u001b[38;5;28mself\u001b[39m.policies = policies\n\u001b[32m   3534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3535\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3536\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`policies` must be dict mapping PolicyID to PolicySpec OR a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3537\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mset/tuple/list of PolicyIDs!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3538\u001b[39m         )\n\u001b[32m   3540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m algorithm_config_overrides_per_module != DEPRECATED_VALUE:\n\u001b[32m   3541\u001b[39m     deprecation_warning(old=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, error=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: `policies` must be dict mapping PolicyID to PolicySpec OR a set/tuple/list of PolicyIDs!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-14 16:36:13,677 E 446246 446246] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-14 16:36:15,545 E 446344 446344] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(pid=446398)\u001b[0m [2025-11-14 16:36:18,704 E 446398 446463] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
    "\n",
    "Author: Rohan (https://github.com/Rohan138)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import ray\n",
    "import supersuit as ss\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "from torch import nn\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "\n",
    "class CNNModelV2(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
    "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            (nn.Linear(3136, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_fn = nn.Linear(512, num_outputs)\n",
    "        self.value_fn = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
    "        self._value_out = self.value_fn(model_out)\n",
    "        return self.policy_fn(model_out), state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self._value_out.flatten()\n",
    "\n",
    "\n",
    "def env_creator(args):\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        n_pistons=20,\n",
    "        time_penalty=-0.1,\n",
    "        continuous=True,\n",
    "        random_drop=True,\n",
    "        random_rotate=True,\n",
    "        ball_mass=0.75,\n",
    "        ball_friction=0.3,\n",
    "        ball_elasticity=1.5,\n",
    "        max_cycles=125,\n",
    "    )\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.dtype_v0(env, \"float32\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    return env\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ray.init()\n",
    "\n",
    "    env_name = \"pistonball_v6\"\n",
    "\n",
    "    def _make_rllib_env(config):\n",
    "        base = env_creator(config)\n",
    "        wrapped = ParallelPettingZooEnv(base)\n",
    "        wrapped._agent_ids = set(getattr(base, \"possible_agents\", []))\n",
    "        return wrapped\n",
    "\n",
    "    register_env(env_name, _make_rllib_env)\n",
    "\n",
    "    ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(\n",
    "            env=env_name,\n",
    "            clip_actions=True,\n",
    "            disable_env_checking=True,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size=512,\n",
    "            lr=2e-5,\n",
    "            gamma=0.99,\n",
    "            lambda_=0.9,\n",
    "            use_gae=True,\n",
    "            clip_param=0.4,\n",
    "            grad_clip=None,\n",
    "            entropy_coeff=0.1,\n",
    "            vf_loss_coeff=0.25,\n",
    "            num_sgd_iter=10,\n",
    "        )\n",
    "        .debugging(log_level=\"ERROR\")\n",
    "        .framework(framework=\"torch\")\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "        .multi_agent(\n",
    "            policies = \"learning_policy\",\n",
    "            policy_mapping_fn = 'learning_policy',\n",
    "            policies_to_train=[\"learning_policy\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    storage_uri = (Path(\"~/ray_results\") / env_name).expanduser().resolve().as_uri()\n",
    "\n",
    "    tune.run(\n",
    "        \"PPO\",\n",
    "        name=\"PPO\",\n",
    "        stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
    "        checkpoint_freq=10,\n",
    "        storage_path=storage_uri,\n",
    "        config=config.to_dict(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d490f-78e2-4130-a33e-688a4210baa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
